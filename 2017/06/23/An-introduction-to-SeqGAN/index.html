<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>An introduction to SeqGAN | 生生 - Jiaqi Fu's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">An introduction to SeqGAN</h1><a id="logo" href="/.">生生 - Jiaqi Fu's Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">An introduction to SeqGAN</h1><div class="post-meta">Jun 23, 2017</div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Previous-Problems"><span class="toc-number">1.</span> <span class="toc-text">Previous Problems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-Nets"><span class="toc-number">2.</span> <span class="toc-text">Proposed Nets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy-Gradient"><span class="toc-number">3.</span> <span class="toc-text">Policy Gradient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiments"><span class="toc-number">4.</span> <span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#References"><span class="toc-number">5.</span> <span class="toc-text">References</span></a></li></ol></div></div><div class="post-content"><p>In this article, I make an introduction of <strong>SeqGAN</strong> (<strong>AAAI2017</strong>) [<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489" target="_blank" rel="external">Link</a>]. In this research, <strong>GAN</strong> has internally combined with <strong>RL</strong> (<strong>Reinforcement Learning</strong>) rules as <strong>SeqGAN</strong> to generate discrete sequences. </p>
<h3 id="Previous-Problems"><a href="#Previous-Problems" class="headerlink" title="Previous Problems"></a>Previous Problems</h3><p>RNN is utilized to process the machine learning task on sequential data. Generally, give <strong>previous data</strong>, we train RNN to maximize the <strong>log predictive likelihood</strong> of each true token in the training sequence.</p>
<p>However, it can suffer what is called <strong>exposure bias</strong> in that the model would output a sequence which has never been in the training set.</p>
<p>There are two researches to address the problem:</p>
<ul>
<li><strong>Schedule Sampleing</strong> (<strong>SS</strong>)[1]. Partially feeding $G$ with synthetic data as prefix. But [2] showed that SS is an inconsistent training strategy and not is able to address the problem fundamentally. </li>
<li><strong>BLEU</strong>[3]. A task specific sequence score/Loss for entire generated sequence instead of each transition. Nevertheless, it&#x2019;s limited for full content.</li>
</ul>
<p>Thus, the author decides to leverage <strong>GAN</strong> in that it&#x2019;s widely used in image/sentences generation. However, to address discrete sequences, <strong>two problems</strong> are faced with current GAN:</p>
<ul>
<li>GAN is designed for <strong>continuous data</strong>. It&#x2019;s called as <strong>slight change</strong> when $G$ changes slightly as $D$ sends back a slight $\nabla_{\mathrm{loss}}$. But it&#x2019;s not appropriate for discrete data.</li>
<li>GAN can only give a score for the full generated sequence.</li>
</ul>
<h3 id="Proposed-Nets"><a href="#Proposed-Nets" class="headerlink" title="Proposed Nets"></a>Proposed Nets</h3><p>As problems discussed in the previous chapter, the author puts forward <strong>SeqGAN</strong>. SeqGAN is consisted of <strong>GAN</strong> and <strong>Reinforcement Learning</strong></p>
<p>The <strong>state</strong>, <strong>reward</strong> and <strong>action</strong> are key elements in RL. In SeqGAN, the $G$ is seen as the <strong>agent</strong>. The <strong>state</strong> is generated tokens so far and the <strong>action</strong> is the next token. </p>
<p>To solve the problem for discrete data, the author regard the $G$ as a <strong>stochastic parametrized policy</strong>. In policy gradient, Monte Carlo Tree Search (MCTS) is used.</p>
<p>The diagram of SeqGAN is showed:</p>
<center><img src="/2017/06/23/An-introduction-to-SeqGAN/1.png" alt=""></center>

<p>As we can see, the RL <strong>reward</strong> signal comes from the GAN discriminator judged on a complete sequence, and is to be sent back to the intermediate state-action steps using <strong>MCTS</strong>.</p>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>Assume a sequence $Y_{1:T}=(y_1,\cdots,y_t,\cdots,y_T)$, $y_t \in \mathcal{Y}$, where $\mathcal{Y}$ is vocabulary. We train a $\theta$-parameterized policy model $G_\theta$. To predict the next token at $t$, the policy model $G_\theta(y_t | Y_{1:t-1})$ is stochastic.</p>
<p>As for a complete sequence, the <strong>objective function</strong> of the policy ($G$) is:</p>
<p>$$<br>\begin{eqnarray}<br>J(\theta) = \mathbb E [R_T | s_0 ,\theta] = \sum_{y_1 \in \mathcal{Y}} G_\theta (y_1 | s_0) \cdot Q_{D_\phi}^{G_\theta} (s_0 , y_1)<br>\end{eqnarray}<br>$$</p>
<p>In above formula, $Q(s|a)$ is the action-value. The author estimate it by:</p>
<p>$$<br>\begin{eqnarray}<br>Q_{D_\phi}^{G_\theta} (s=Y_{1:T-1} , a=y_T)  = D_\phi (Y_1:T)<br>\end{eqnarray}<br>$$</p>
<p>Moreover, the objective of SeqGAN is to address <strong>unfinished</strong> sequential learning. Thus, the author uses the <strong>Mente Carlo Tree Search</strong> with a roll-out policy $G_\beta$ to sample the rest of the sequence. Here is $N$ times search by MCTS:</p>
<p>$$<br>\begin{eqnarray}<br>\left\{ Y_{1:T}^1, \cdots , Y_{1:T}^N \right\} = \mathrm{MC}^{G_\beta} (Y_{1:t};N)<br>\end{eqnarray}<br>$$</p>
<p>Thus, we have (Eq. 4 in the original paper):</p>
<p>$$<br>Q_{D_\phi}^{G_\theta} (s=Y_{1:T-1} , a=y_T) = \\<br>\left\{<br>\begin{aligned}<br>\frac{1}{N} \sum_{n=1}^N D_\phi (Y_{1:T}^n),Y_{1:T}^n \in \mathrm{MC}^{G_\beta} (Y_{1:t};N) \quad&amp;\mathrm{for}&amp; \quad t &lt; T\\<br>D_\phi (Y_{1:t}) \quad &amp;\mathrm{for}&amp; \quad t=T<br>\end{aligned}<br>\right.<br>$$</p>
<p>For the discriminator $D_\phi$, it can <strong>dynamically</strong> output <strong>reward</strong> for further improve the $G$. It&#x2019;s same as traditional GAN to have $G$ train after getting realistic generated sequences (Eq. 5 in the original paper):<br>$$<br>\begin{eqnarray}<br>\min_\phi - \mathbb{E}_{Y \sim p_{\mathrm{data}}} [\log D_\phi (Y)] - \mathbb{E}_{Y \sim G_\theta} [\log (1-D_\phi (Y))]<br>\end{eqnarray}<br>$$</p>
<p>The gradient of the <strong>objective function</strong> can be derived as:<br>$$<br>\begin{eqnarray}<br>\nabla_\theta J(\theta) = \mathbb{E}_{Y_{1:t-1} \sim G_\theta} [\sum_{y_t \in \mathcal{Y}} \nabla_\theta G_\theta (y_t | Y_{1:t-1}) \cdot Q_{D_\phi}^{G_\theta}(Y_{1:t-1},y_t)]<br>\end{eqnarray}<br>$$</p>
<p>The detailed derivation is in the paper, here is the final result:</p>
<p>$$<br>\begin{eqnarray}<br>\nabla_\theta J(\theta) \simeq \frac{1}{T} \sum_{t=1}^{T} \sum_{y_t \in \mathcal{Y}} \nabla_{ \theta } G_{\theta} \cdot Q_{D_\phi}^{G_\theta}(Y_{1:t-1},y_t) \\<br>= \frac{1}{T} \sum_{t=1}^{T} \sum_{y_t \in \mathcal{Y}} G_\theta(y_t | Y_{1:t-1}) \nabla_\theta \log G_\theta(y_t | Y_{1:t-1}) \cdot Q_{D_\phi}^{G_\theta}(Y_{1:t-1},y_t) \\<br>= \frac{1}{T} \sum_{t=1}^{T} \mathbb{E}_{y_t \sim G_\theta(y_t | Y_{1:t-1})} [\nabla_\theta \log G_\theta(y_t | Y_{1:t-1}) \cdot Q_{D_\phi}^{G_\theta}(Y_{1:t-1},y_t)]<br>\end{eqnarray}<br>$$</p>
<p>We can update the $\theta$ by (Eq. 8 in the original paper):<br>$$<br>\begin{eqnarray}<br>\theta \leftarrow \theta + \alpha_h \nabla_\theta J(\theta)<br>\end{eqnarray}<br>$$</p>
<p>Here is the diagram of the algorithm:</p>
<center><img src="/2017/06/23/An-introduction-to-SeqGAN/2.png" alt=""></center>

<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>SeqGAN is used to test three real-world scenarios:</p>
<ul>
<li>poem composition</li>
<li>speech language generation</li>
<li>music generation</li>
</ul>
<p>In text generation, the author uses a corpus of 16,394 Chinese quatrains. And in the Obama political speech generation task, the 11,092 paragraphs are accustomed.</p>
<p><strong>BLEU</strong> score is one of evaluation metrics. In the meanwhile, they invited 70 experts on Chinese poems and let them do <strong>Turing Test</strong>.</p>
<p>In music generation, 695 midi files are utilized. They extract <strong>one track</strong>, and use 88 pitches. <strong>BLEU</strong> is invoked as the evaluation metric.</p>
<p>Here is the result of three scenarios:</p>
<center><img src="/2017/06/23/An-introduction-to-SeqGAN/3.png" alt=""></center>

<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li>Bengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015. Scheduled sampling for sequence predic- tion with recurrent neural networks. In NIPS, 1171&#x2013;1179.</li>
<li>Husza &#x301;r, F. 2015. How (not) to train your gen- erative model: Scheduled sampling, likelihood, adversary? arXiv:1511.05101.</li>
<li>Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic eval- uation of machine translation. In ACL, 311&#x2013;318.</li>
</ol>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://fujiaqi.com/2017/06/23/An-introduction-to-SeqGAN/" data-id="cj49qp1v00005kvln0tbnxslk" class="article-share-link">Share</a><div class="tags"><a href="/tags/Machine-Learning/">Machine Learning</a><a href="/tags/GAN/">GAN</a></div><div class="post-nav"><a href="/2017/05/31/ACL16-17-civil-research-institution/" class="next">Chinese Research Institutions in ACL 17&amp;16</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://fujiaqi.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/C/" style="font-size: 15px;">C++</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Music-Computing/" style="font-size: 15px;">Music Computing</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/Algorithm/" style="font-size: 15px;">Algorithm</a> <a href="/tags/GAN/" style="font-size: 15px;">GAN</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/06/23/An-introduction-to-SeqGAN/">An introduction to SeqGAN</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/31/ACL16-17-civil-research-institution/">Chinese Research Institutions in ACL 17&16</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/31/AI-Drummer-Demo/">AI Drummer Demo</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/05/03/Notes-in-generating-MIDI/">Notes in generating MIDI(PPT)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/18/An-introduction-to-C-RNN-GAN-Generating-Music/">An introduction to C-RNN-GAN (Generating Music)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/12/Note-Introduction-and-Applications-of-GANs-PPT/">Note: Introduction and Applications of GANs(PPT)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/06/GAN-2-Introduction-and-Interpretation-of-Paper/">GAN (2): Introduction via the Original Paper</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/04/05/GAN-Generative-Model-and-Discrimination-Model/">GAN (1): Generative Model and Discrimination Model</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/17/Spring-MVC-Login-Demo/">Spring MVC Login Demo</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/13/Non-Linear SVM and Kernel Function非线性支持向量机与核函数/">Non-Linear SVM and Kernel Function非线性支持向量机与核函数</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://keefe.wang/" title="keefe's Blog" target="_blank">keefe's Blog</a><ul></ul><a href="https://github.com/umbrellabeach" title="Jiaqi Fu's Github" target="_blank">Jiaqi Fu's Github</a><ul></ul><a href="http://mypage.zju.edu.cn/en/zhangkejun" title="Kejun Zhang's Page" target="_blank">Kejun Zhang's Page</a><ul></ul><a href="http://huisblog.cn" title="huisblog" target="_blank">huisblog</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">生生 - Jiaqi Fu's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-97246531-1','auto');ga('send','pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>